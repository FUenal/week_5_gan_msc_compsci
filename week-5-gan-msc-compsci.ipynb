{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Mini-Project - I’m Something of a Painter Myself: Use GANs to create art - will you be the next Monet?\n\n**Author:** Fatih Uenal\n\n**Course:** CU Boulder MSc Computer Science & AI\n\n**Date:** 12.06.2025\n\n**GitHub Repository:** [Github](https://github.com/FUenal/week_5_gan_msc_compsci)\n\n-----","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nGenerative Adversarial Networks (GANs) are a powerful deep learning approach used for generating new data that mimics a given dataset. In this project, we will apply GANs to the task of artistic style transfer. Specifically, we aim to transform real-world photographs into paintings in the style of Claude Monet, using the \"gan-getting-started\" dataset from Kaggle. This project offers a hands-on opportunity to implement and train a CycleGAN model, a state-of-the-art architecture for image-to-image translation.\n\n## Problem Statement\n\nThe primary objective is to build, train, and evaluate a GAN model capable of generating Monet-style paintings from input photographs. The model's performance will be assessed based on Kaggle's Memorization-Informed Fréchet Inception Distance (MiFID) metric, which evaluates both the quality of the generated images and their originality. A lower MiFID score indicates a better-performing model.\n\n## Dataset Overview\n\n* **Dataset Name:** gan-getting-started\n* **Source:** [Kaggle \"I'm Something of a Painter Myself\" Competition](https://www.kaggle.com/c/gan-getting-started)\n* **Dataset Structure:**\n    * **Monet Paintings:** A collection of Claude Monet's paintings.\n    * **Photos:** A set of real-world photographs to be translated into the Monet style.\n* **Data Format:** The images are provided in TFRecord format, with a uniform resolution of 256x256 pixels.\n\n## Objectives\n\n1.  Construct and train a CycleGAN model to generate Monet-style paintings from photographs.\n2.  Optimize the model for high performance on Kaggle's public leaderboard.\n3.  Generate a submission file (`images.zip`) containing 7,000-10,000 generated images.\n\n## Deliverables\n\nThis notebook provides the complete code and explanation to fulfill the project's objectives, including data loading, model definition, training, and image generation for submission.\n\n---","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport os\nimport zipfile\nimport time\n\n# TPU Configuration\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(\"✅ TPU connected\")\nexcept:\n    strategy = tf.distribute.get_strategy()\n    print(\"⚠️ TPU not found, using default strategy\")\n\nREPLICAS = strategy.num_replicas_in_sync\nAUTO = tf.data.AUTOTUNE\nprint(f\"REPLICAS: {REPLICAS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:38:26.105885Z","iopub.execute_input":"2025-06-11T15:38:26.106167Z","iopub.status.idle":"2025-06-11T15:38:50.161047Z","shell.execute_reply.started":"2025-06-11T15:38:26.106143Z","shell.execute_reply":"2025-06-11T15:38:50.155495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----\n\n# Exploratory Data Analysis (EDA)\n\nBefore we build our CycleGAN, let's explore the two datasets to understand their characteristics. We'll look at the number of images and visualize some examples. We will also analyze the color distributions to see if there are noticeable differences in the color palettes between Monet's paintings and the real-world photographs.\n\n-----","metadata":{}},{"cell_type":"code","source":"# Loading and Preprocessing Data\nGCS_PATH = KaggleDatasets().get_gcs_path('gan-getting-started')\nMONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\nn_monet_samples = len(MONET_FILENAMES)\nn_photo_samples = len(PHOTO_FILENAMES)\n\nIMAGE_SIZE = [256, 256]\nBATCH_SIZE = 1 * REPLICAS\n\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef data_augment(image):\n    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    image = tf.image.random_crop(image, size=[256, 256, 3])\n    image = tf.image.random_flip_left_right(image)\n    return image\n\ndef read_tfrecord(example, augmented):\n    tfrecord_format = { \"image\": tf.io.FixedLenFeature([], tf.string) }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if augmented:\n        image = data_augment(image)\n    return image\n\ndef configure_dataset(filenames, augmented=True, shuffle=True, repeat=True):\n    ds = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    ds = ds.map(lambda x: read_tfrecord(x, augmented), num_parallel_calls=AUTO)\n    if shuffle:\n        ds = ds.shuffle(n_monet_samples * 2)\n    if repeat:\n        ds = ds.repeat()\n    ds = ds.batch(BATCH_SIZE, drop_remainder=True)\n    ds = ds.prefetch(AUTO)\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:38:54.201694Z","iopub.execute_input":"2025-06-11T15:38:54.202205Z","iopub.status.idle":"2025-06-11T15:38:54.246086Z","shell.execute_reply.started":"2025-06-11T15:38:54.202176Z","shell.execute_reply":"2025-06-11T15:38:54.240831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper function for EDA data loading\ndef load_eda_dataset(filenames):\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.map(lambda x: read_tfrecord(x, augmented=False), num_parallel_calls=AUTO)\n    return dataset\n    \n# Loading Datasets and getting Sizes\nmonet_ds_eda = load_eda_dataset(MONET_FILENAMES)\nphoto_ds_eda = load_eda_dataset(PHOTO_FILENAMES)\n\nprint(f\"Number of Monet paintings: {n_monet_samples}\")\nprint(f\"Number of photos: {n_photo_samples}\")\n\n#Visualizinh sample Images\ndef display_samples(dataset, title, n_samples=5):\n    plt.figure(figsize=(20, 4))\n    for i, img in enumerate(dataset.take(n_samples)):\n        plt.subplot(1, n_samples, i + 1)\n        plt.imshow(img * 0.5 + 0.5)\n        plt.title(f\"{title} #{i+1}\")\n        plt.axis(\"off\")\n    plt.show()\n\nprint(\"\\nSample Monet Paintings:\")\ndisplay_samples(monet_ds_eda, \"Monet\")\n\nprint(\"\\nSample Photographs:\")\ndisplay_samples(photo_ds_eda, \"Photo\")\n\n# Feature Space Analysis\nprint(\"\\nAnalyzing dataset difference in VGG19 Feature Space...\")\n\nwith strategy.scope():\n    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=[*IMAGE_SIZE, 3])\n    vgg.trainable = False\n    vgg_feature_extractor = tf.keras.Model(vgg.input, vgg.get_layer('block4_conv4').output)\n\ndef extract_features(dataset, n_samples=100):\n    features = []\n    for i, img in enumerate(dataset.take(n_samples)):\n        img_denormalized = (img + 1) * 127.5\n        feature_map = vgg_feature_extractor(tf.expand_dims(img_denormalized, 0))\n        features.append(np.mean(feature_map, axis=(1, 2)).flatten())\n    return np.array(features)\n\n# Extracting features\nmonet_features = extract_features(monet_ds_eda, n_samples=n_monet_samples)\nphoto_features = extract_features(photo_ds_eda, n_samples=n_monet_samples) # Use same sample size for balance\n\n# Using PCA to reduce feature dimensions to 2D for plotting\npca = PCA(n_components=2)\nall_features = np.concatenate([monet_features, photo_features])\npca_features = pca.fit_transform(all_features)\n\n# Plotting the feature clusters\nplt.figure(figsize=(10, 8))\nplt.scatter(pca_features[:n_monet_samples, 0], pca_features[:n_monet_samples, 1], label='Monet Paintings', alpha=0.7)\nplt.scatter(pca_features[n_monet_samples:, 0], pca_features[n_monet_samples:, 1], label='Photographs', alpha=0.7)\nplt.title('VGG19 Feature Space of Monet vs. Photo Datasets (PCA)')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.grid(True, linestyle='--')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:38:56.763909Z","iopub.execute_input":"2025-06-11T15:38:56.764215Z","iopub.status.idle":"2025-06-11T15:39:06.686087Z","shell.execute_reply.started":"2025-06-11T15:38:56.764191Z","shell.execute_reply":"2025-06-11T15:39:06.680811Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Discussion of EDA Results\n\nThe exploratory data analysis reveals several key insights that directly inform our modeling strategy:\n\n1.  **Visual Inspection:** A side-by-side look at the images confirms the obvious: the domains are stylistically very different. Monet's works are characterized by visible brushstrokes, a unique color palette, and an emphasis on light over sharp detail. The photographs are realistic and high-fidelity.\n\n2.  **Feature Space Separation:** The most telling analysis comes from the PCA plot of the VGG19 feature space. We can clearly see two distinct clusters, with very little overlap between the Monet paintings and the real-world photographs. This is a crucial finding. It demonstrates that, from the perspective of a powerful deep learning model, the two datasets are perceptually very far apart.","metadata":{}},{"cell_type":"markdown","source":"---\n\n# **Model Choice and Strategy**\n\nFor this task, the foundational choice is the **Cycle-Consistent Generarial Adversarial Network (CycleGAN)**. This architecture is the industry standard for unpaired image-to-image translation and is uniquely suited to this problem, as I do not have direct \"photo-to-painting\" pairs. The core principles of CycleGAN I leverage are:\n\n1.  **Unpaired Translation:** The model learns the general characteristics of two image domains (in our case, photos and Monet paintings) and finds a mapping between them without needing one-to-one examples.\n2.  **Dual Architecture:** The model uses two Generators and two Discriminators simultaneously. One pair learns the `Photo -> Monet` translation, while the other learns the reverse (`Monet -> Photo`).\n3.  **Cycle-Consistency Loss:** This is the key innovation. By making sure that an image translated to the other domain and back again arrives close to the original (`Photo -> Monet -> Photo' ≈ Photo`), the model is forced to preserve the content of the image while only changing its artistic style.\n\nHowever, the Exploratory Data Analysis (EDA) revealed that a standard CycleGAN would likely be insufficient to achieve a top-tier result. The **Feature Space Analysis** showed a significant perceptual gap between the photo and Monet domains (PCA Analysis). This insight directly informed my decision to enhance the standard CycleGAN with more advanced techniques:\n\n* **Enhanced Loss Function with Perceptual Loss:** To bridge the large perceptual gap identified in the EDA, I have augmented the standard losses with a **Perceptual Loss**. This loss utilizes a pre-trained VGG19 network to extract high-level feature representations of the generated and target images. By minimizing the difference between these feature maps, I force the generator to learn not just the correct colors, but also the complex textures, patterns, and brushstroke styles that make a Monet painting perceptually unique. This is the key component for achieving a realistic \"painterly\" feel.\n\n* **U-Net Based Generator:** For the generator architecture, I've implemented a **U-Net**. Its characteristic skip connections connect the downsampling path to the upsampling path, allowing low-level information (like composition and object edges) to bypass the bottleneck. This is crucial for generating a new style while ensuring the final image is still a faithful representation of the original photo's content.\n\n* **Refined Training Strategy:** To ensure stable convergence and allow the model to refine fine-grained details in later stages of training, I will employ a **Learning Rate Scheduler**. This implementation keeps the learning rate constant for the first half of the epochs for rapid initial learning, then linearly decays it to zero, helping the model to settle into a high-quality local minimum without instability.\n\nIn summary, my final model is not a standard CycleGAN, but an **enhanced, perceptually-aware network** with a custom loss function and a refined training strategy, all of which are directly motivated by our deep analysis of the dataset.\n\n-------","metadata":{}},{"cell_type":"code","source":"# Model Architecture (CycleGAN)\nclass InstanceNormalization(tf.keras.layers.Layer):\n    def __init__(self, epsilon=1e-5):\n        super(InstanceNormalization, self).__init__()\n        self.epsilon = epsilon\n    def build(self, input_shape):\n        self.scale = self.add_weight(name='scale', shape=input_shape[-1:], initializer=tf.random_normal_initializer(1., 0.02), trainable=True)\n        self.offset = self.add_weight(name='offset', shape=input_shape[-1:], initializer='zeros', trainable=True)\n    def call(self, x):\n        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n        inv = tf.math.rsqrt(variance + self.epsilon)\n        normalized = (x - mean) * inv\n        return self.scale * normalized + self.offset\n\ndef downsample(filters, size, apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n    if apply_instancenorm:\n        result.add(InstanceNormalization())\n    result.add(layers.LeakyReLU())\n    return result\n\ndef upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n    result.add(InstanceNormalization())\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n    result.add(layers.ReLU())\n    return result\n\ndef Generator():\n    inputs = layers.Input(shape=[256, 256, 3])\n    down_stack = [\n        downsample(64, 4, False), downsample(128, 4), downsample(256, 4), downsample(512, 4),\n        downsample(512, 4), downsample(512, 4), downsample(512, 4), downsample(512, 4),\n    ]\n    up_stack = [\n        upsample(512, 4, True), upsample(512, 4, True), upsample(512, 4, True),\n        upsample(512, 4), upsample(256, 4), upsample(128, 4), upsample(64, 4),\n    ]\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=initializer, activation='tanh')\n    x = inputs\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    skips = reversed(skips[:-1])\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n    x = last(x)\n    return keras.Model(inputs=inputs, outputs=x)\n\ndef Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n    down1 = downsample(64, 4, False)(inp)\n    down2 = downsample(128, 4)(down1)\n    down3 = downsample(256, 4)(down2)\n    zero_pad1 = layers.ZeroPadding2D()(down3)\n    conv = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)\n    norm1 = InstanceNormalization()(conv)\n    leaky_relu = layers.LeakyReLU()(norm1)\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n    last = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)\n    return tf.keras.Model(inputs=inp, outputs=last)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:39:11.829486Z","iopub.execute_input":"2025-06-11T15:39:11.829821Z","iopub.status.idle":"2025-06-11T15:39:11.855299Z","shell.execute_reply.started":"2025-06-11T15:39:11.829795Z","shell.execute_reply":"2025-06-11T15:39:11.848576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model, Optimizers, and Loss Functions\nwith strategy.scope():\n    # Perceptual Loss\n    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=[*IMAGE_SIZE, 3])\n    vgg.trainable = False\n    perceptual_loss_model = tf.keras.Model(vgg.input, vgg.get_layer('block4_conv4').output)\n\n    def perceptual_loss_fn(real, fake):\n        real_features = perceptual_loss_model((real + 1) * 127.5)\n        fake_features = perceptual_loss_model((fake + 1) * 127.5)\n        loss = tf.reduce_mean(tf.square(real_features - fake_features), axis=[1, 2, 3])\n        return 5e-3 * tf.nn.compute_average_loss(loss, global_batch_size=BATCH_SIZE)\n\n    # Learning rate scheduler\n    EPOCHS = 40\n    STEPS_PER_EPOCH = n_monet_samples // BATCH_SIZE\n    DECAY_START_EPOCH = int(EPOCHS * 0.5)\n\n    class LinearDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n        def __init__(self, initial_learning_rate, total_steps, step_decay):\n            super(LinearDecay, self).__init__()\n            self.initial_learning_rate = tf.cast(initial_learning_rate, tf.float32)\n            self.total_steps = tf.cast(total_steps, tf.float32)\n            self.step_decay = tf.cast(step_decay, tf.float32)\n        def __call__(self, step):\n            step_float = tf.cast(step, tf.float32)\n            return tf.cond(\n                step_float < self.step_decay,\n                lambda: self.initial_learning_rate,\n                lambda: self.initial_learning_rate - (self.initial_learning_rate * (step_float - self.step_decay) / (self.total_steps - self.step_decay))\n            )\n\n    # initializing Models, Optimizers, and Standard Loss Functions\n    monet_generator = Generator()\n    photo_generator = Generator()\n    monet_discriminator = Discriminator()\n    photo_discriminator = Discriminator()\n    lr_schedule = LinearDecay(2e-4, EPOCHS * STEPS_PER_EPOCH, DECAY_START_EPOCH * STEPS_PER_EPOCH)\n    monet_generator_optimizer = tf.keras.optimizers.Adam(lr_schedule, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(lr_schedule, beta_1=0.5)\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(lr_schedule, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(lr_schedule, beta_1=0.5)\n    bce_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n    def discriminator_loss_fn(r, g): return tf.nn.compute_average_loss((bce_loss_fn(tf.ones_like(r), r) + bce_loss_fn(tf.zeros_like(g), g)) * 0.5, BATCH_SIZE)\n    def generator_loss_fn(g): return tf.nn.compute_average_loss(bce_loss_fn(tf.ones_like(g), g), BATCH_SIZE)\n    def cycle_loss_fn(r, c): return 10.0 * tf.nn.compute_average_loss(tf.reduce_mean(tf.abs(r - c), axis=[1,2,3]), BATCH_SIZE)\n    def identity_loss_fn(r, s): return 10.0 * 0.5 * tf.nn.compute_average_loss(tf.reduce_mean(tf.abs(r - s), axis=[1,2,3]), BATCH_SIZE)\n\n# TRAINING STEP FUNCTION (with all losses)\nwith strategy.scope():\n    @tf.function\n    def train_step(real_monet, real_photo):\n        with tf.GradientTape(persistent=True) as tape:\n            fake_monet = monet_generator(real_photo, training=True)\n            cycled_photo = photo_generator(fake_monet, training=True)\n            fake_photo = photo_generator(real_monet, training=True)\n            cycled_monet = monet_generator(fake_photo, training=True)\n            same_monet = monet_generator(real_monet, training=True)\n            same_photo = photo_generator(real_photo, training=True)\n            disc_real_monet = monet_discriminator(real_monet, training=True)\n            disc_real_photo = photo_discriminator(real_photo, training=True)\n            disc_fake_monet = monet_discriminator(fake_monet, training=True)\n            disc_fake_photo = photo_discriminator(fake_photo, training=True)\n\n            # Generator losses\n            monet_gen_adv_loss = generator_loss_fn(disc_fake_monet)\n            photo_gen_adv_loss = generator_loss_fn(disc_fake_photo)\n            perceptual_loss = perceptual_loss_fn(real_monet, fake_monet)\n            total_cycle_loss = cycle_loss_fn(real_monet, cycled_monet) + cycle_loss_fn(real_photo, cycled_photo)\n            \n            total_monet_gen_loss = monet_gen_adv_loss + total_cycle_loss + identity_loss_fn(real_monet, same_monet) + perceptual_loss\n            total_photo_gen_loss = photo_gen_adv_loss + total_cycle_loss + identity_loss_fn(real_photo, same_photo)\n\n            # Discriminator losses\n            monet_disc_loss = discriminator_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = discriminator_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculating and applying gradients\n        m_gen_grads = tape.gradient(total_monet_gen_loss, monet_generator.trainable_variables)\n        p_gen_grads = tape.gradient(total_photo_gen_loss, photo_generator.trainable_variables)\n        m_disc_grads = tape.gradient(monet_disc_loss, monet_discriminator.trainable_variables)\n        p_disc_grads = tape.gradient(photo_disc_loss, photo_discriminator.trainable_variables)\n        \n        monet_generator_optimizer.apply_gradients(zip(m_gen_grads, monet_generator.trainable_variables))\n        photo_generator_optimizer.apply_gradients(zip(p_gen_grads, photo_generator.trainable_variables))\n        monet_discriminator_optimizer.apply_gradients(zip(m_disc_grads, monet_discriminator.trainable_variables))\n        photo_discriminator_optimizer.apply_gradients(zip(p_disc_grads, photo_discriminator.trainable_variables))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:39:14.219676Z","iopub.execute_input":"2025-06-11T15:39:14.220022Z","iopub.status.idle":"2025-06-11T15:39:15.542486Z","shell.execute_reply.started":"2025-06-11T15:39:14.219995Z","shell.execute_reply":"2025-06-11T15:39:15.535695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper function to display results after each epoch.\ndef display_generated_samples(generator, photo_dataset):\n    for photo_batch in photo_dataset:\n        prediction = generator(photo_batch, training=False)[0].numpy()\n        plt.figure(figsize=(12, 12))\n        original_photo = photo_batch[0]\n        display_list = [original_photo, prediction]\n        title = ['Input Photo', 'Generated Monet']\n        for i in range(2):\n            plt.subplot(1, 2, i+1)\n            plt.title(title[i])\n            # Un-normalize the image from [-1, 1] to [0, 1] for display\n            plt.imshow(display_list[i] * 0.5 + 0.5)\n            plt.axis('off') \n        plt.show()\n        break\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:39:26.945047Z","iopub.execute_input":"2025-06-11T15:39:26.945354Z","iopub.status.idle":"2025-06-11T15:39:26.957061Z","shell.execute_reply.started":"2025-06-11T15:39:26.945328Z","shell.execute_reply":"2025-06-11T15:39:26.953318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Loop\nEPOCHS = 20\nSTEPS_PER_EPOCH = n_monet_samples // BATCH_SIZE\n\nmonet_ds = configure_dataset(MONET_FILENAMES)\nphoto_ds = configure_dataset(PHOTO_FILENAMES)\nfinal_ds = strategy.experimental_distribute_dataset(tf.data.Dataset.zip((monet_ds, photo_ds)))\nphoto_ds_vis = configure_dataset(PHOTO_FILENAMES, augmented=False, shuffle=False).take(1)\n\ndef display_generated_samples(generator, photo_batch):\n    prediction = generator(photo_batch, training=False)[0].numpy()\n    plt.figure(figsize=(12, 12))\n    display_list = [photo_batch[0], prediction]\n    title = ['Input Photo', 'Generated Monet']\n    for i in range(2):\n        plt.subplot(1, 2, i+1); plt.title(title[i]); plt.imshow(display_list[i] * 0.5 + 0.5); plt.axis('off')\n    plt.show()\n\nprint(\"Starting final professional-grade training... 🚀\")\nfor epoch in range(EPOCHS):\n    start_time = time.time()\n    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n    progbar = tf.keras.utils.Progbar(STEPS_PER_EPOCH, unit_name='step')\n    for step, batch in enumerate(final_ds):\n        if step >= STEPS_PER_EPOCH: break\n        strategy.run(train_step, args=batch)\n        progbar.update(step + 1)\n    print(f\"\\nDisplaying sample result for epoch {epoch+1}:\")\n    for photo_batch in photo_ds_vis:\n        display_generated_samples(monet_generator, photo_batch)\n        break\n    print(f\"Time for epoch {epoch + 1} is {time.time()-start_time:.2f} sec\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:39:30.290086Z","iopub.execute_input":"2025-06-11T15:39:30.290386Z","iopub.status.idle":"2025-06-11T15:42:06.900772Z","shell.execute_reply.started":"2025-06-11T15:39:30.290362Z","shell.execute_reply":"2025-06-11T15:42:06.895270Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n# Results and Discussion\n\nThe final model was trained for 20 epochs using our enhanced CycleGAN architecture. The primary goal was to move beyond simple color transfer and generate images with authentic, Monet-like texture and brushstrokes. The results shows a clear success in this objective.\n\n## Qualitative Analysis\n\nAs seen in the sample image generated at the end of training, the model successfully learned to translate a standard photograph into a vibrant, impressionistic painting. Key achievements include:\n\n* **Texture and Brushstrokes:** The most significant improvement over baseline models is the emergence of a \"painterly\" texture. Instead of a blurry or smudged effect, the output image has complex patterns that mimic the short, thick brushstrokes characteristic of Monet's work. This is a direct result of incorporating the **Perceptual Loss**, which forced the generator to learn high-level stylistic features.\n* **Color Palette:** The model accurately captured the Monet color palette, shifting the realistic tones of the photograph to the brighter, more vibrant blues, greens, and yellows found in Impressionist art.\n* **Content Preservation:** Despite the dramatic stylistic transformation, the core content and composition of the original photograph are  preserved, thanks to the **Cycle-Consistency Loss**. The bridge, trees, and water are all discernible.\n\nThe learning process, visualized by generating a sample after each epoch, showed a rapid progression. The model quickly moved past a blurry initial state and began developing complex textures within the first 10-15 epochs, stabilizing and refining them as training progressed.\n\n## Training Process\n\nThe training process itself remained stable, which is a common challenge with GANs. The use of a **manual training loop** with a well-defined `train_step` provided robustness. The **Learning Rate Scheduler**, set to begin decaying after 20 epochs, is designed to help the model further refine these details in longer training runs, preventing the kind of plateau we observed in earlier experiments.\n\n---","metadata":{}},{"cell_type":"markdown","source":"---\n\n# Conclusion\n\nThis project developed and trained an enhanced Generative Adversarial Network to perform artistic style transfer, converting real-world photographs into paintings in the style of Claude Monet.\n\nBy systematically iterating on our approach, I demonstrated that a standard CycleGAN, while effective, could be significantly improved. The final model incorporated three key professional-grade techniques: **Data Augmentation**, a **Perceptual Loss** function (using a pre-trained VGG19 network), and a **Learning Rate Scheduler**.\n\nThe final generated images exhibit a rich, painterly texture and a vibrant color palette that successfully captures the essence of Monet's style, moving beyond a simple filter. This confirms that a carefully designed loss function that operates on a perceptual level is crucial for achieving high-fidelity results in complex style transfer tasks. The project successfully met all its objectives and produced a powerful and effective model for artistic image generation.\n\n---","metadata":{}},{"cell_type":"markdown","source":"------\n\n# Future Work and Potential Improvements\n\nWhile the current model is successful, there are several potentialy ways for future experimentation and improvement:\n\n1.  **ResNet-Based Generator:** The other state-of-the-art architecture for this task uses a generator built with Residual Blocks (ResNet) instead of a U-Net. Implementing a ResNet generator would be a logical next step to explore a different style of painting, one that might focus even more on preserving the original photo's structure.\n2.  **Hyperparameter Tuning:** The weights of the various loss functions (adversarial, cycle, identity, and perceptual) were set to standard values. A rigorous hyperparameter search could yield a better balance, potentially creating even more pronounced textures or more accurate colors. For instance, increasing the weight of the perceptual loss could be explored.\n3.  **Longer Training:** The current results are from 20 epochs. Running the model for the full 40 epochs (to take advantage of the learning rate decay) or even longer (80-100 epochs) would likely lead to further refinement and detail in the generated images.\n4.  **Application to Other Artists:** The final pipeline is robust and could be readily adapted to learn the style of other famous artists, such as Van Gogh, Cézanne, or Picasso, by simply swapping the Monet dataset for a collection of their works.\n\n-----","metadata":{}},{"cell_type":"code","source":"# Image generating for sbmission\nimport PIL\n\nprint(\"\\nTraining finished. Generating images for submission... 🎨\")\nos.makedirs(\"/kaggle/working/images\", exist_ok=True)\n\ndef configure_dataset_final(filenames, augmented=False, shuffle=False, repeat=False, drop_remainder=False):\n    ds = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    ds = ds.map(lambda x: read_tfrecord(x, augmented), num_parallel_calls=AUTO)\n    if shuffle:\n        ds = ds.shuffle(n_monet_samples * 2)\n    if repeat:\n        ds = ds.repeat()\n    ds = ds.batch(BATCH_SIZE, drop_remainder=drop_remainder)\n    ds = ds.prefetch(AUTO)\n    return ds\n    \nsubmission_ds = configure_dataset_final(PHOTO_FILENAMES, augmented=False, shuffle=False, repeat=False, drop_remainder=False)\n\nimg_count = 0\nfor img_batch in submission_ds:\n    for img in img_batch:\n        prediction = monet_generator(tf.expand_dims(img, 0), training=False)[0].numpy()\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        im = PIL.Image.fromarray(prediction)\n        img_count += 1\n        im.save(f'/kaggle/working/images/{img_count}.jpg')\n\nprint(f\"Generated {img_count} images.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T16:57:20.277768Z","iopub.execute_input":"2025-06-11T16:57:20.278171Z","iopub.status.idle":"2025-06-11T17:11:00.430467Z","shell.execute_reply.started":"2025-06-11T16:57:20.278145Z","shell.execute_reply":"2025-06-11T17:11:00.425314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating submission Zip file\nzip_file_path = '/kaggle/working/images.zip'\nwith zipfile.ZipFile(zip_file_path, 'w') as zf:\n    # Iterate over the generated image files\n    for filename in os.listdir('/kaggle/working/images'):\n        file_path = os.path.join('/kaggle/working/images', filename)\n        # Write the file to the zip archive, using only the filename as the archive name\n        zf.write(file_path, arcname=filename)\n\nprint(f\"\\nSubmission file created at: {zip_file_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:11:19.061593Z","iopub.execute_input":"2025-06-11T17:11:19.061979Z","iopub.status.idle":"2025-06-11T17:11:20.086000Z","shell.execute_reply.started":"2025-06-11T17:11:19.061947Z","shell.execute_reply":"2025-06-11T17:11:20.082421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}